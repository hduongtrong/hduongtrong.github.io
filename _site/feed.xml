<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hoang Duong blog</title>
    <description>Learning Machine Learning</description>
    <link>http://hduongtrong.github.io/</link>
    <atom:link href="http://hduongtrong.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 20 Nov 2015 16:44:13 -0800</pubDate>
    <lastBuildDate>Fri, 20 Nov 2015 16:44:13 -0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Word Representation - Word2Vec</title>
        <description>&lt;h2 id=&quot;credits&quot;&gt;Credits&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Original authors: &lt;a href=&quot;http://arxiv.org/abs/1301.3781&quot;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean.&lt;/li&gt;
&lt;li&gt;Code Website: &lt;a href=&quot;https://code.google.com/p/word2vec/&quot;&gt;Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Explained Paper: &lt;a href=&quot;http://u.cs.biu.ac.il/%7Enlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf&quot;&gt;Neural Word Embedding as Implicit Matrix Factorization&lt;/a&gt;. Omer Levy, Yoav Goldberg&lt;/li&gt;
&lt;li&gt;Explained Paper Short Version: &lt;a href=&quot;http://arxiv.org/abs/1402.3722&quot;&gt;Word2vec Explained&lt;/a&gt;. Yoav Goldberg, Omer Levy&lt;/li&gt;
&lt;li&gt;Code easier to understand: &lt;a href=&quot;https://github.com/fchollet/keras/blob/master/examples/skipgram_word_embeddings.py&quot;&gt;Keras Word2vec&lt;/a&gt;. Francois Chollet. &lt;/li&gt;
&lt;li&gt;TensorFlow code: &lt;a href=&quot;http://tensorflow.org/tutorials/word2vec/index.md&quot;&gt;TensorFlow word2vec&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If your time is in short supply, just read the Explained Paper Short Version. &lt;/p&gt;

&lt;h2 id=&quot;goals&quot;&gt;Goals&lt;/h2&gt;

&lt;p&gt;English language has in the order of 100,000 words. If we are working on an NLP problem, one can represent each word as a one-hot vector of dimension 100,000. This is a sparse and high dimension input. Our goal is to map this into dense low dimensional input of around 300 dimension, \( v _w\). Then we can feed this to some other model like LSTM for some NLP tasks. Hopefully, our new representation respects some semantic rules like &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Words with similar meaning should be close to each other&lt;/li&gt;
&lt;li&gt;The direction from &amp;quot;do&amp;quot; to &amp;quot;doing&amp;quot; should be similar to &amp;quot;go&amp;quot; to &amp;quot;going&amp;quot;&lt;/li&gt;
&lt;li&gt;Doing addition: King - male + female ~ queen&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is surprising that word2vec can give us this, without any label on the meaning of words. All it needs is words cooccurance. It is also surpsisingly simple, nothing more complicated than logistic regression.&lt;/p&gt;

&lt;h2 id=&quot;ideas-of-word2vec&quot;&gt;Ideas of Word2vec&lt;/h2&gt;

&lt;p&gt;Given a sentence: &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You used to call me on my cell phone. &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;and a window k (consider k = 1 for simplicity), we define the context of a word as its 2k neighbor words. Word2vec defines the positive dataset \( \mathcal{D} \) of &lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;word&lt;/th&gt;
&lt;th&gt;context&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;you&lt;/td&gt;
&lt;td&gt;used&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;used&lt;/td&gt;
&lt;td&gt;you&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;used&lt;/td&gt;
&lt;td&gt;to&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;to&lt;/td&gt;
&lt;td&gt;used&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;to&lt;/td&gt;
&lt;td&gt;call&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;Each of these pair did appear in the dataset, so we associate them with a label 1. We now define the negative word pairs dataset \( \mathcal{D&amp;#39;} \) &lt;/p&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;word&lt;/th&gt;
&lt;th&gt;non_context&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;you&lt;/td&gt;
&lt;td&gt;random_word1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;used&lt;/td&gt;
&lt;td&gt;random_word2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;to&lt;/td&gt;
&lt;td&gt;random_word3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;call&lt;/td&gt;
&lt;td&gt;random_word4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;and label these pair as 0 (Note that when we pick one random word from the vocabulary, there is some tiny chance that the picked word is actually a valid context, but it is very small that we consider it 0).  &lt;/p&gt;

&lt;p&gt;We then use the logistic loss to train a vector representation for each word such as it maximize
$$ \arg \max _{\theta} \prod _{(w,c) \in \mathcal{D}}  p(D _{w,c} = 1 \mid w,c,\theta) 
                       \prod _{(w,c) \in \mathcal{D&amp;#39;}} p(D _{w,c} = 0 \mid w,c,\theta).$$
So we basically maximize the probability of seeing those word pairs in \( \mathcal{D}\), and not seeing those word pairs in \( \mathcal{D&amp;#39;}\). Taking log, we can rewrite the optimization as 
$$ \arg \max _{\theta} \sum _{(w,c) \in \mathcal{D }} \log \sigma(  v _c \cdot v _w) + 
                       \sum _{(w,c) \in \mathcal{D&amp;#39;}} \log \sigma(- v _c \cdot v _w),$$
for \( \sigma(x) = \frac{1}{1 + e ^ {-x}} \)&lt;/p&gt;

&lt;p&gt;Now to be clear, all of the \( v _w\) are our vector representation of word, together, they form a matrix of size (vocabulary size by projection size), e.g. (100,000 by 300). All of the \( v _c \) are our vector representation of context, together, they form a matrix of similar size if we have one negative sample per positive sample. In practice this later matrix is often discarded. We basically optimize with respect to those two matrix, such that the product \( v _c \cdot v _w\) is big for those [word, context] we see in our dataset, and small for those [word, context] we do not see in our dataset. &lt;/p&gt;

&lt;h2 id=&quot;enhancement&quot;&gt;Enhancement&lt;/h2&gt;

&lt;p&gt;The final method used is a bit more complicated, with tricks that make it work better, for example&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Instead of sampling one negative context per [word, context] pair uniformly, it samples \(m\) context words with probability distribution proportional to how often each context word is in the dataset.&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Instead of using a fix \( k\) window around each word, the window is uniformly distributed from \( 1,2, ..., K \)&lt;/li&gt;
&lt;li&gt;Treat all rare words as an &amp;quot;UNK&amp;quot; token, and downsampling most common words. &lt;/li&gt;
&lt;li&gt;The method we mention so far is called the Skip-Gram Negative Sampling, the original paper also mentions the Continuous Bag of Words, where it models the probability of word given context.  The authors claim that the skip-gram negative sampling works better for bigger dataset (see more in the TensorFlow example). &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;extra&quot;&gt;Extra&lt;/h2&gt;

&lt;p&gt;This word representation, finding the two vector (matrix) representation \( V _w\) and \( V _c\) can be thought of as factorizing an implicit matrix \( M = V _w \cdot V _c ^ T \), where each element of \( M, M _{ij}\) reflect the strength of association between word \( i \) and context \( j\). More specifically, it is found that 
$$ M _{ij} = v _w \cdot v _c = PMI(w _i, c _j) - \log k$$, 
for PMI is the pointwise mutual information, defined as 
$$PMI(w _i, c _j) = \log \left( \frac { N(w,c) |\mathcal{D}|}{ N(w) N(c) } \right)$$, 
for \( N(w)\) counts the number of occurance of \(w  \) in \( \mathcal{D}\)&lt;/p&gt;

&lt;h2 id=&quot;testing-out&quot;&gt;Testing out&lt;/h2&gt;

&lt;p&gt;The official website of word2vec has a very fast code in C++, where one can test things out pretty easily. Head over there, training the model should take five minutes. It even has the option of download pre-trained models. Here I&amp;#39;m using there pre-trained model on Google News words, and find similar words to a word (think of this like a Thesaurus)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Word: eminem  Position in vocabulary: 566691&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;        Word       Cosine distance
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;       jay z      0.732697
   rick_ross      0.731509
       kanye      0.715282
        shyt      0.705407
 chris brown      0.697447
       i luv      0.694622
   lady gaga      0.690142
  john mayer      0.686606
        ozzy      0.678592
  soulja boy      0.671136
     rihanna      0.670854
        juss      0.670568
   lil wayne      0.669848
     beyonce      0.667990
       cuz u      0.664925
      mariah      0.664813
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/blockquote&gt;
</description>
        <pubDate>Fri, 20 Nov 2015 07:28:27 -0800</pubDate>
        <link>http://hduongtrong.github.io/2015/11/20/word2vec/</link>
        <guid isPermaLink="true">http://hduongtrong.github.io/2015/11/20/word2vec/</guid>
        
        
      </item>
    
      <item>
        <title>Deep Reinforcement Learning</title>
        <description>&lt;p&gt;(INCOMPLETE)
Deep Reinforcement Learning is an exciting new field that encompasses many
different fields: computer science, optimal control, statistics, machine
learning, and so on. Its application are numerous.&lt;/p&gt;

&lt;h2 id=&quot;policy-gradient&quot;&gt;Policy Gradient&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; A Markov Decision Process contains&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\( \pi : \mathcal{S} \rightarrow \Delta(\mathcal(A)) \), the stochastic policy. &lt;/li&gt;
&lt;li&gt;\( \eta(\pi) = \mathbb{E} \left[ R_0 + \gamma R_1 + \gamma&lt;sup&gt;2&lt;/sup&gt; R_2 +&lt;br&gt;
... \right] \)&lt;/li&gt;
&lt;li&gt;\( p: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R} \), the state transition probability&lt;/li&gt;
&lt;li&gt;\( \mu : \mathcal{S} \rightarrow \mathbb{R} \), the probability distribution over the initial state, \( s_0 \)&lt;/li&gt;
&lt;li&gt;\( \theta \in \mathbb{R}&lt;sup&gt;n&lt;/sup&gt; \), a vector of parameter that parameterizes the stochastic policy \(\pi\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A policy gradient algorithm then calculate \( \nabla_ {\theta} \eta (\theta) \), and make proceed as a standard gradient ascent algorithm. We approximate the gradient using Monte Carlo estimation, since we don&amp;#39;t have access to the underlying probability distribution. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; Monte Carlo estimation. Let \( X : \Omega \rightarrow \mathbb{R} ^ n \) be a random variable with probability distribution \(q\), and \(f : \mathbb {R}&lt;sup&gt;n&lt;/sup&gt; \rightarrow \mathbb{R} \). Then 
    $$ \frac{\partial}{\partial \theta } \mathbb {E} \left[ f(X) \right] = 
        \mathbb{E} \left[ f(X) \frac{\partial}{\partial \theta } 
        \log q(X;\theta) \right]$$&lt;/p&gt;

&lt;p&gt;Armed with this theorem, we can use a sample average to estimate the expectation. &lt;/p&gt;
</description>
        <pubDate>Mon, 19 Oct 2015 18:28:27 -0700</pubDate>
        <link>http://hduongtrong.github.io/2015/10/19/deep-reinforcement-learning/</link>
        <guid isPermaLink="true">http://hduongtrong.github.io/2015/10/19/deep-reinforcement-learning/</guid>
        
        
      </item>
    
      <item>
        <title>How to make a blog like this</title>
        <description>&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;I have been seeing beautiful blog posts coupled with great contents such as &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html&quot;&gt;The Zen of Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Understanding LSTM Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My definition of being &amp;quot;beautiful&amp;quot; is just text and no distraction, look
good both on mobile and on desktop, and display Latex. My definition of &amp;quot;great
content&amp;quot; often involve machine learning, optimization, and related topics. &lt;/p&gt;

&lt;h2 id=&quot;setting-up&quot;&gt;Setting up&lt;/h2&gt;

&lt;p&gt;So without being further ado, let&amp;#39;s start with how to make a blog like those
three mentioned above blog. You will need a couple components:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;: Pretty easy to install on Mac, just follow the
link&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://pages.gith%E2%80%A6&quot;&gt;Github Pages&lt;/a&gt;: Also pretty easy to setup. Create a
github account if you haven&amp;#39;t got one, then follow the link.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://help.github.com/articles/markdown-basics/&quot;&gt;Markdown&lt;/a&gt;: Used to generate html file from simpler looking file (no need to install this)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.mathjax.org&quot;&gt;Mathjax&lt;/a&gt;: For typing Latex. No need to install this either&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After spending some good hours and failing at setting up the various 
requirements, here is the trick I found: just clone the blog of the people
above. In particular, I clone Karpathy blog into mine. Many thanks to Andre
Karpathy for this, and my apology if my blog accidentally contains some stuffs
from yours. Ok now open &lt;strong&gt;Terminal&lt;/strong&gt; in Mac:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;mkdir MyBlog
&lt;span class=&quot;c&quot;&gt;# Downloading sample github page from Andre Karpathy&lt;/span&gt;
git clone https://github.com/karpathy/karpathy.github.io.git
&lt;span class=&quot;c&quot;&gt;# Downloading your own github page blog. Replacing username with your github&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# user name&lt;/span&gt;
git clone https://github.com/username/username.github.io.git
cp ./karpathy.github.io/* ./username.github.io/
&lt;span class=&quot;c&quot;&gt;# Removing stuff associated with the original owner&lt;/span&gt;
rm nntutorial.md
rm _posts/* &lt;span class=&quot;c&quot;&gt;# Hist posts&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You will need to change a few more information. Open &lt;code&gt;_config.yaml&lt;/code&gt;_ and change
all the detail in there to your username. Change things in &lt;code&gt;about.md&lt;/code&gt; as well. 
Also in &lt;code&gt;_layouts/post.html&lt;/code&gt; and
&lt;code&gt;_layouts/page.html&lt;/code&gt;, there is a part with &amp;quot;karpathy&amp;quot;, change that to your
username. Basically, search for all text with the original owner&amp;#39;s name and
replace them with your name. You might also want to delete everything in i
&lt;code&gt;assets&lt;/code&gt;, as those are the picture that the original owner uses.  &lt;/p&gt;

&lt;p&gt;And voila, you are done.&lt;/p&gt;

&lt;h2 id=&quot;hello-world&quot;&gt;Hello World&lt;/h2&gt;

&lt;p&gt;To create your first blog post, again copy over a sample post say from&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;cp ./karpathy.github.io/_posts/2015-05-21-rnn-effectiveness.markdown
   ./username.github.io/_posts/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And you can start changing things in there. Then do this &lt;code&gt;jekyll serve&lt;/code&gt; inside
your &lt;code&gt;username.github.io&lt;/code&gt; folder. You should see something like this if
successful &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Configuration file: /Users/hd/Documents/Blog/hduongtrong.github.io/_config.yml
            Source: /Users/hd/Documents/Blog/hduongtrong.github.io
       Destination: /Users/hd/Documents/Blog/hduongtrong.github.io/_site
      Generating... 
     Build Warning: Layout &amp;#39;none&amp;#39; requested in feed.xml does not exist.
                    done.
 Auto-regeneration: enabled for &amp;#39;/Users/hd/Documents/Blog/hduongtrong.github.io&amp;#39;
Configuration file: /Users/hd/Documents/Blog/hduongtrong.github.io/_config.yml
    Server address: http://127.0.0.1:4000/
  Server running... press ctrl-c to stop.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Open your web browser, and go to &lt;code&gt;http://127.0.0.1:4000/&lt;/code&gt;, you will see your
post there. &lt;/p&gt;

&lt;p&gt;Now to push this online, just &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;username.github.io/
git add .
git commit -m &lt;span class=&quot;s2&quot;&gt;&amp;quot;First blog&amp;quot;&lt;/span&gt;
git push origin master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And go to &lt;code&gt;username.github.io&lt;/code&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Oct 2015 09:46:27 -0700</pubDate>
        <link>http://hduongtrong.github.io/2015/10/19/how-setting-up-blog/</link>
        <guid isPermaLink="true">http://hduongtrong.github.io/2015/10/19/how-setting-up-blog/</guid>
        
        
      </item>
    
  </channel>
</rss>

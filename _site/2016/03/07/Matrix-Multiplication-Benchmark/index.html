<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Matrix Multiplication Benchmark</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Learning Machine Learning">
    <link rel="canonical" href="http://hduongtrong.github.io/2016/03/07/Matrix-Multiplication-Benchmark/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-3698471-23', 'auto');
      ga('send', 'pageview');
    </script>

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <!div style="float:left; margin-top:10px; margin-right:10px;">
    <!a href="/feed.xml">
      <!img src="/assets/rssicon.svg" width="40">
    <!/a>
    <!/div>

    <a class="site-title" href="/">Hoang Duong blog</a>
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Matrix Multiplication Benchmark</h1>
    <p class="meta">Mar 7, 2016</p>
  </header>

  <article class="post-content">
  <h2 id="the-setting">The setting</h2>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">();</span> <span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">);</span> <span class="k">print</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">a</span>
</code></pre></div>
<p>The contestants</p>

<table><thead>
<tr>
<th style="text-align: right">CPU</th>
<th style="text-align: right">Freq</th>
<th style="text-align: right">N_Cores</th>
<th style="text-align: right">L3_Cache</th>
<th style="text-align: right">Date</th>
<th style="text-align: right">Price</th>
<th style="text-align: right">Passmark</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: right">Intel Core i5-4260U</td>
<td style="text-align: right">1.4GHz</td>
<td style="text-align: right">2</td>
<td style="text-align: right">3MB</td>
<td style="text-align: right">Q2-14</td>
<td style="text-align: right">315</td>
<td style="text-align: right">3548</td>
</tr>
<tr>
<td style="text-align: right">Intel Xeon E5-2643 v2</td>
<td style="text-align: right">3.5GHz</td>
<td style="text-align: right">2x6=12</td>
<td style="text-align: right">2x25MB</td>
<td style="text-align: right">Q3-13</td>
<td style="text-align: right">2x1552</td>
<td style="text-align: right">2x11735</td>
</tr>
<tr>
<td style="text-align: right">AMD Opteron 8384</td>
<td style="text-align: right">2.7GHz</td>
<td style="text-align: right">8x4=32</td>
<td style="text-align: right">8x6MB</td>
<td style="text-align: right">Q4-08</td>
<td style="text-align: right">8x2149</td>
<td style="text-align: right">NA</td>
</tr>
<tr>
<td style="text-align: right">AMD Opteron 8272</td>
<td style="text-align: right">1.4Ghz</td>
<td style="text-align: right">2x8=16</td>
<td style="text-align: right">2x6MB</td>
<td style="text-align: right">Q4-11</td>
<td style="text-align: right">2x523</td>
<td style="text-align: right">2x6748</td>
</tr>
</tbody></table>

<h2 id="mkl-vs-openblas">MKL vs OpenBlas</h2>

<p>Here are the running time in seconds. The number in () are roughly the fluctuation of running time. For the GPU result, Tesla K80 is a dual GPU, and this is only using one of them, which is equivalent to Tasla K40. In addition, calculation is carried out with float64, which GPU is bad at. Note for non-mkl, we use the default Blas library on OS X El Capitan. For the other, they are Openblas. MKL in general gives more variable results, but slightly better than the non-MKL on Intel CPUs. </p>

<table><thead>
<tr>
<th style="text-align: right">CPU</th>
<th style="text-align: right">Non-MKL</th>
<th style="text-align: right">MKL</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: right">Intel Core i5-4260U</td>
<td style="text-align: right">43</td>
<td style="text-align: right">32</td>
</tr>
<tr>
<td style="text-align: right">Intel Xeon E5-2643 v2</td>
<td style="text-align: right">15.6</td>
<td style="text-align: right">10.4 (3)</td>
</tr>
<tr>
<td style="text-align: right">AMD Opteron 8384</td>
<td style="text-align: right">15.4 (2)</td>
<td style="text-align: right">12.3 (1)</td>
</tr>
<tr>
<td style="text-align: right">AMD Opteron 8272</td>
<td style="text-align: right">17.3</td>
<td style="text-align: right">22 (5)</td>
</tr>
<tr>
<td style="text-align: right">Tesla K-80</td>
<td style="text-align: right">16.3</td>
<td style="text-align: right"></td>
</tr>
</tbody></table>

<h2 id="cpu-vs-gpu">CPU vs GPU</h2>

<p>To really see the power of GPU, we use float32 instead.</p>

<table><thead>
<tr>
<th style="text-align: right">Matrix dim</th>
<th style="text-align: right">CPU</th>
<th style="text-align: right">GPU Tensorflow</th>
<th style="text-align: right">GPU Skcuda</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: right">10000</td>
<td style="text-align: right">6.3</td>
<td style="text-align: right">2.3</td>
<td style="text-align: right">1.3</td>
</tr>
<tr>
<td style="text-align: right">15000</td>
<td style="text-align: right">17</td>
<td style="text-align: right">6.8</td>
<td style="text-align: right">3.7</td>
</tr>
<tr>
<td style="text-align: right">20000</td>
<td style="text-align: right">39</td>
<td style="text-align: right">10.8</td>
<td style="text-align: right">8.32</td>
</tr>
<tr>
<td style="text-align: right">30000</td>
<td style="text-align: right">122</td>
<td style="text-align: right">NA</td>
<td style="text-align: right">27.0</td>
</tr>
</tbody></table>

<p>GPU only provides a speed up of around 4-5 times. The GPU 1 is done by Tensorflow, which might not be very efficient. The GPU 2 is done by Scikit-cuda, which is a wrapper for pycuda. For the later one, we also see a breakdown of communication time between CPU and GPU. It spends around 15% of the time copying data in and out of GPU. </p>

<p>Tools for doing linear algebra on GPU.  </p>

<ol>
<li>Pycuda: this is the lowest level, a wrapper of CUDA for Python</li>
<li>Scikit-cuda: a wrapper over pycuda</li>
<li>Cula: provide LAPACK type of matrix multiplication for CUDA</li>
<li>Numbapro / accelerate: from Anaconda</li>
<li>Theano / Tensorflow</li>
</ol>

<h2 id="codes">Codes</h2>

<h4 id="skcuda-pycuda">Skcuda + Pycuda</h4>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pycuda.gpuarray</span> <span class="kn">as</span> <span class="nn">gpuarray</span>
<span class="kn">import</span> <span class="nn">pycuda.autoinit</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span><span class="o">,</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">skcuda.linalg</span> <span class="kn">as</span> <span class="nn">culinalg</span>
<span class="kn">import</span> <span class="nn">skcuda</span>
<span class="n">culinalg</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="n">dim</span> <span class="o">=</span> <span class="mi">30000</span>
<span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">rnd</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">a_gpu</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&#39;Copy in&#39;</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">rescpu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&#39;CPU:&#39;</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">resgpu</span> <span class="o">=</span> <span class="n">culinalg</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a_gpu</span><span class="p">,</span> <span class="n">a_gpu</span><span class="p">)</span>
<span class="k">print</span> <span class="n">skcuda</span><span class="o">.</span><span class="n">misc</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">resgpu</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&#39;GPU:&#39;</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">resgpu</span> <span class="o">=</span> <span class="n">resgpu</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="k">print</span> <span class="s">&#39;Copy out&#39;</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
<span class="k">print</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">rescpu</span><span class="p">,</span> <span class="n">resgpu</span><span class="p">)</span>
<span class="k">print</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">resgpu</span><span class="p">,</span> <span class="n">rescpu</span><span class="p">)</span>
</code></pre></div>
<h4 id="tensorflow">Tensorflow</h4>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20000</span><span class="p">,</span><span class="mi">20000</span><span class="p">),</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">()</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="k">print</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">a</span>
</code></pre></div>
<h4 id="cpu-both-openblas-and-mkl">CPU (both OpenBLAS and MKL)</h4>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">30000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">),</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">();</span> <span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">);</span> <span class="k">print</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">a</span>
</code></pre></div>
  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  
  <!-- disqus comments -->
 
 <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'HoangDT Blog'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  
  <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'andersonvom'; // required: replace example with your forum shortname
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">Hoang Duong blog</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>Hoang Duong blog</li>
        <!-- <li><a href="mailto:hduong@berkeley.edu">hduong@berkeley.edu</a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/hduongtrong">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">hduongtrong</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/hduongtrong">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">hduongtrong</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Learning Machine Learning</p>
    </div>

  </div>

</footer>


    </body>
</html>
<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Decision Trees, Random Forest, Gradient Boosting</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Learning Machine Learning">
    <link rel="canonical" href="http://hduongtrong.github.io/2016/04/04/Trees-Based-Models/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-3698471-23', 'auto');
      ga('send', 'pageview');
    </script>

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <!div style="float:left; margin-top:10px; margin-right:10px;">
    <!a href="/feed.xml">
      <!img src="/assets/rssicon.svg" width="40">
    <!/a>
    <!/div>

    <a class="site-title" href="/">Hoang Duong blog</a>
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Decision Trees, Random Forest, Gradient Boosting</h1>
    <p class="meta">Apr 4, 2016</p>
  </header>

  <article class="post-content">
  <h2 id="decision-tree">1. Decision Tree</h2>

<h3 id="intro">1.1. Intro</h3>

<p>Decision trees sound straightforward enough, as for example seen in Figure 1. It is however not that straightforward to implement. We try to do it here.</p>

<div class="imgcap">
<div>
<img src="/assets/tree/rpart.plot-example1.png" width="300" />
</div>
<div class="thecap">Figure 1: An example of a decision tree - Credit: RPart </div>
</div>

<h3 id="algorithm">1.2. Algorithm</h3>
<p>Assuming we are doing classification, \(y \in {0,1} ^ n\), and \(X \in \mathbb{R} ^ {n \times d}\)</p>

<p>A decision tree starts as follow:</p>

<ol>
  <li>
    <p>For each of the d features in X, rank \( X _ i\) and sort the y according to that feature \( X _ i\). First calculate the Gini score for the sorted y, which is 
 <script type="math/tex">Gini = p(1 - p)</script>
 for \(p\) is the proportion of 1 in \(y \). Now we calculate the two Gini score for each of the \(n - 1\) possible splits. As follow 
 <script type="math/tex">Gini _ i = \frac{i}{n} p _ {i1} (1 - p _ {i1}) + \frac{n - i}{n} p _ {i2} (1 - p _ {i2}),</script>
 for here \(p _ {i1}\) is the proportion of 1 in y_sorted[:i], and \(p _ {i2}\) is the proportion of 1 in y_sorted[i:], using numpy notation. Subtracting these \(n - 1\) Gini score by the total Gini score to get the information gain for each of the split. 
 At the end, we have the best information gain out of all possible \( d(n - 1)\) splits. We pick the best split and move one to the next step.</p>
  </li>
  <li>
    <p>In this second steps, we have two nodes, each with \(n _ 1, n _ 2\) observations respectively. For each nodes, we proceed as in Step 1 and split according to the best out of \(d (n _ i -  1) \) splits, for \(i \in \{1, 2 \}\).</p>
  </li>
  <li>
    <p>We keep the same procedure until some pre-specified max depth, for example 8.</p>
  </li>
</ol>

<p>For regression, instead of using Gini coefficient, we use the Variance. Overall, the Gini coefficient and variance measure the purity of a node. These scores are small if all observations within the nodes are very similar to each other, and big if the observations are very different from each other. A decision tree wants this because it will classify all observation in each node to be the majority class, or in case of regression predicting all observation to be the mean.</p>

<h3 id="computation-complexity">1.3. Computation Complexity</h3>

<p>For step 1, for each feature, getting the \(n - 1\) Gini coefficient is \(O(n) \). Sorting the feature is \( O(n\log n)\). So the total cost is \(O( nd \log n)\). We proceed this step 1 until some max depth. The max depth is in the order of \( O (\log n)\). So in total, the complexity should be \( O (nd (\log n) ^ 2)\). Overall, it is pretty efficient. Note that for some variant, we donâ€™t consider all \(d\) feature at each split, but only a portion, this might reduce the complexity a bit further.</p>

<p>The codes to get the \(n - 1\) Gini score efficiently in \(O(n)\) is as followed.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">GetP</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="s">""" This function return the proportion of 1 in y[:i], 
	for i from 1 to length(y)
    Parameters:
    -----------
        y: numpy binary 1D array
    Returns:
    --------
        numpy float 1D array of length n - 1, containing proportion of 1
    """</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">GetGini</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="s">""" Return the Gini score of y for each way of splitting, 
        i.e. Gini( y[:1], y[1:] ), Gini(y[:2], y[2:], ...)
    """</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">GetP</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">GetP</span><span class="p">(</span><span class="n">y</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">p1</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">p2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span> <span class="o">/</span> <span class="n">n</span>
</code></pre>
</div>

<h3 id="hyper-parameters">1.4. Hyper Parameters</h3>

<p>For a tree, the important hyperparameters control its complexity</p>

<ol>
  <li>Max Depth: the lower it is, the less complex. It should not be much higher than \( \log _ 2 n \)</li>
  <li>Min Sample Nodes: each node should contain at least this many samples</li>
  <li>Min Sample split: to consider split a node, the node must have at least this many samples (note that this is different from 2.)</li>
  <li>Max number of nodes: this is straight forward, and is roughly equal to \(2 ^ {maxdepth}\).</li>
  <li>Criterion: for classification, in addition to Gini score, one can use entropy, which is \(p \log p + (1 - p) \log (1 - p)\). The two functions are in fact very similar</li>
</ol>

<h2 id="ensembles">2. Ensembles</h2>

<h3 id="random-forest">2.1. Random Forest</h3>
<p>Once we understand decision tree, random forest is a piece of cake. It basically do</p>

<ol>
  <li>Sample \(\alpha n, \alpha \in [0,1]\) observations from the original dataset, and \(\beta p, \beta \in [0,1]\) features, and build a tree here</li>
  <li>Repeat the process for k times, and classify observations according to majority, or regression observation according to the mean.</li>
</ol>

<p>Three more hyper parameters that we introduced:</p>

<ol>
  <li>Proportion of observation to sample: the smaller it is, the more independent the different trees are</li>
  <li>Proportion of feature to sample: similar to 1. For classification, it is recommended to be around \(\sqrt{d}\), for regression, it is recommended to be \(d / 3\)</li>
  <li>Number of trees: basically the bigger the better, but just more computation.</li>
</ol>

<p>Each of the trees in Random Forest is independent of each other, and thus can be embarrassingly paralleled. In general, the technique of averaging many small models, where each model use a bootstrap sample of the data, is called bagging (bootstrap aggregation).</p>

<h3 id="gradient-boosting-machine">2.2. Gradient Boosting Machine</h3>

<p>The algorithm is as followed (credit ESL)</p>

<ol>
  <li>Initialize \(f _ 0 (x) = \arg \min _ {\gamma} \sum _ {i = 1} L(y _ i, \gamma) \)</li>
  <li>For m = 1 to M:
    <ol>
      <li>For \(i = 1, 2, â€¦, N\) compute 
 <script type="math/tex">r _ {im} = -\left[ \frac{\partial L(y _ i, f(x _ i))}{\partial f (x _ i)}\right] _ {f = f _ {m - 1}}</script></li>
      <li>Fit a regression tree to the target \(r _ {im}\) giving terminal regions \(R _ {jm}, j = 1, 2, â€¦, J _ m.\)</li>
      <li>For \(j = 1, 2, â€¦, J _ m\) compute <script type="math/tex">\gamma _ {jm} = \arg \min \sum _ {x _ i \in R _ {jm} } L(y _ i, f _ {m - 1} (x _ i) + \gamma)</script></li>
      <li>Update \( f _ m (x) = f _ {m - 1} (x) + \sum _ {j = 1} ^ {J _ m} \gamma _ {jm} I(x \in R _ {jm})\)</li>
    </ol>
  </li>
  <li>Output \(\hat{f} (x) = f _ M (x).\)</li>
</ol>

<p>For regression with loss function \(L(y, \gamma) = (y - \gamma) ^ 2\), we have</p>

<ol>
  <li>Initialize \(f _ 0 (x) = \bar{y} \)</li>
  <li>For m = 1 to M:
    <ol>
      <li>For \(i = 1, 2, â€¦, N\) compute 
 <script type="math/tex">r _ {im} = y _ i - f _ {m - 1} (x _ i)</script></li>
      <li>Fit a regression tree to the target \(r _ {im}\) giving terminal regions \(R _ {jm}, j = 1, 2, â€¦, J _ m.\)</li>
      <li>For \(j = 1, 2, â€¦, J _ m\) compute <script type="math/tex">\gamma _ {jm} = \bar{y} _ {jm} - f _ {m - 1} (x _ i)</script>, for \(\bar{y} _ {jm}\) is the mean of \(y\) in that region</li>
      <li>Update \( f _ m (x) = f _ {m - 1} (x) + \sum _ {j = 1} ^ {J _ m} \gamma _ {jm} I(x \in R _ {jm})\)</li>
    </ol>
  </li>
  <li>Output \(\hat{f} (x) = f _ M (x).\)</li>
</ol>

<p>Basically, we start with a simple regression tree, and calculate the residual. We then fit a model to the residual, and calculate the aggregated model. We calculate the new residual and keep proceeding. 
For classification, the deviance loss function can be used.</p>

<p>Some improvements</p>

<ol>
  <li>Use shrinkage in Step 2.4. The update rule is \( f _ m (x) = f _ {m - 1} (x) + \nu \sum _ {j = 1} ^ {J _ m} \gamma _ {jm} I(x \in R _ {jm})\), for \(\nu \in (0, 1]\). This is similar to a learning rate. Lower learning rate needs more iteration (more trees)</li>
  <li>Subsample observations, and columns</li>
</ol>

<p>For hyperparameters, in addition to the 3 hyperparameters as found in Random Forest, Gradient Boosting Machine has 1 additional important hyperparameter which is learning rate. Typically, lower learning rate is better for testing error, but should be accompanied with more trees.</p>

<p>Unlike Random Forest, Gradient Boosting is not easily paralleled. It is still however still doable at each step. More specifically one can use multiple cores to speed up the building of each tree. In some sense Gradient Boosting is similar to Neural Network training with gradient descent. There is both a learning rate, and early stopping. It is not parallelizable along the iteration, but each iteration can be parallelized.</p>

<p>Gradient Boosting is also similar to Stagewise Regression, in that it fits to the residual iteratively at each step.</p>

<h2 id="implementations">3. Implementations</h2>
<p>In R, decision tree is implemented in <em>rpart</em>, while in Python, one can use <em>scikit-klearn</em>. Since we rarely use decision tree, but more often the ensembles, we talk about these more here. Random Forest and Gradient Boosting have their official packages built by the original algorithm of the inventor in R (Leo Breiman and Jerome Friedman). These two packages have a lot of parameters to tweak; however, they are slow and only use 1 core.</p>

<p>In Python, RandomForest of Scikit-Learn is fast and uses multiple core. GBM is slow here however. In both R and Python, one can use <strong>XGBoost</strong> to run Random Forest and GBM really fast and on multiple cores. The algorithm in XGBoost is actually slightly different than GBM. According to the author</p>

<blockquote>
  <p>Both xgboost and gbm follows the principle of gradient boosting.  There are however, the difference in modeling details. Specifically,  xgboost used a <strong>regularized</strong> model formalization to control over-fitting, which gives it better performance.</p>
</blockquote>

<p>To be specific, the loss function is added with an <strong>L2 term</strong> \( \frac{1}{2} \lambda \sum _ {j = 1} ^ T w _ j ^ 2\), where \(w _ j\) is the tree prediction for each leaf. XGBoost is very popular among Kaggle competitors. It is in many of the winning solutions, and can also be used to run Random Forest. In fact, one can have a hybrid of both Random Forest and Gradient Boosting, in that we grow multiple boosted model and averaging them at the end.</p>

<p>Finally, Scikit-learn has an implementation of <strong>Extra Trees</strong> (also called Extremely Randomized Trees). Instead of using the best split for each feature, it uses a <strong>random split for each feature</strong>. This allows the variance to be reduced, and quite often performs better than Random Forest.</p>

<p>In R, <strong>H2O</strong> also provide fast implementation of Random Forest and Gradient Boosting.</p>

<h2 id="references-and-credits">References and Credits</h2>
<ol>
  <li>Element of Statistical Learning</li>
  <li>Tianqi Chen (Author of XGBoost)</li>
  <li>Leo Breiman (Author of CART, Random Forest)</li>
  <li>Jerome Friedman (Author of CART, GBM)</li>
</ol>

  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  
  <!-- disqus comments -->
 
 <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'HoangDT Blog'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  
  <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'andersonvom'; // required: replace example with your forum shortname
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">Hoang Duong blog</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>Hoang Duong blog</li>
        <!-- <li><a href="mailto:hduong@berkeley.edu">hduong@berkeley.edu</a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/hduongtrong">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">hduongtrong</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/hduongtrong">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">hduongtrong</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Learning Machine Learning</p>
    </div>

  </div>

</footer>


    </body>
</html>
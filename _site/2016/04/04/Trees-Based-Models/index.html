<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Decision Trees, Random Forest, Gradient Boosting</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Learning Machine Learning">
    <link rel="canonical" href="http://hduongtrong.github.io/2016/04/04/Trees-Based-Models/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-3698471-23', 'auto');
      ga('send', 'pageview');
    </script>

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <!div style="float:left; margin-top:10px; margin-right:10px;">
    <!a href="/feed.xml">
      <!img src="/assets/rssicon.svg" width="40">
    <!/a>
    <!/div>

    <a class="site-title" href="/">Hoang Duong blog</a>
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">About</a>
        
          
        
          
        
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Decision Trees, Random Forest, Gradient Boosting</h1>
    <p class="meta">Apr 4, 2016</p>
  </header>

  <article class="post-content">
  <h2 id="decision-tree">1. Decision Tree</h2>

<h3 id="intro">1.1. Intro</h3>

<p>Decision trees sound straightforward enough, as for example seen in Figure 1. It is however not that straightforward to implement. We try to do it here.</p>

<div class="imgcap">
<div>
<img src="/assets/tree/rpart.plot-example1.png" />
</div>
<div class="thecap">Figure 1: An example of a decision tree - Credit: RPart </div>
</div>

<h3 id="algorithm">1.2. Algorithm</h3>
<p>Assuming we are doing classification, \(y \in {0,1} ^ n\), and \(X \in \mathbb{R} ^ {n \times d}\)</p>

<p>A decision tree starts as follow:</p>

<ol>
  <li>
    <p>For each of the d features in X, rank \( X _ i\) and sort the y according to that feature \( X _ i\). First calculate the Gini score for the sorted y, which is 
 <script type="math/tex">Gini = p(1 - p)</script>
 for \(p\) is the proportion of 1 in \(y \). Now we calculate the two Gini score for each of the \(n - 1\) possible splits. As follow 
 <script type="math/tex">Gini _ i = \frac{i}{n} p _ {i1} (1 - p _ {i1}) + \frac{n - i}{n} p _ {i2} (1 - p _ {i2}),</script>
 for here \(p _ {i1}\) is the proportion of 1 in y_sorted[:i], and \(p _ {i2}\) is the proportion of 1 in y_sorted[i:], using numpy notation. Subtracting these \(n - 1\) Gini score by the total Gini score to get the information gain for each of the split. 
 At the end, we have the best information gain out of all possible \( d(n - 1)\) splits. We pick the best split and move one to the next step.</p>
  </li>
  <li>
    <p>In this second steps, we have two nodes, each with \(n _ 1, n _ 2\) observations respectively. For each nodes, we proceed as in Step 1 and split according to the best out of \(d (n _ i -  1) \) splits, for \(i \in \{1, 2 \}\).</p>
  </li>
  <li>
    <p>We keep the same procedure until some pre-specified max depth, for example 8.</p>
  </li>
</ol>

<p>For regression, instead of using Gini coefficient, we use the Variance. Overall, the Gini coefficient and variance measure the purity of a node. These scores are small if all observations within the nodes are very similar to each other, and big if the observations are very different from each other. A decision tree wants this because it will classify all observation in each node to be the majority class, or in case of regression predicting all observation to be the mean.</p>

<h3 id="computation-complexity">1.3. Computation Complexity</h3>

<p>For step 1, for each feature, getting the \(n - 1\) Gini coefficient is \(O(n) \). Sorting the feature is \( O(n\log n)\). So the total cost is \(O( nd \log n)\). We proceed this step 1 until some max depth. The max depth is in the order of \( O (\log n)\). So in total, the complexity should be \( O (nd (\log n) ^ 2)\). Overall, it is pretty efficient. Note that for some variant, we don’t consider all \(d\) feature at each split, but only a portion, this might reduce the complexity a bit further.</p>

<p>The codes to get the \(n - 1\) Gini score efficiently in \(O(n)\) is as followed.</p>

<p>```python
from <strong>future</strong> import division
import numpy as np</p>

<p>def GetP(y):
	“”” This function return the proportion of 1 in y[:i], for i from 1 to length(y)
	Parameters:
	———–
		y: numpy binary 1D array
	Returns:
	——–
		numpy float 1D array of length n - 1, containing proportion of 1
	“””
    n = len(y)
    return (y.cumsum()[:-1] / np.arange(1,n))</p>

<p>def GetGini(y):
	“”” Return the Gini score of y for each way of splitting, 
		i.e. Gini( y[:1], y[1:] ), Gini(y[:2], y[2:], …)
	“””
    n = len(y)
    p1 = GetP(y)
    p2 = GetP(y[::-1])[::-1]
    res = np.arange(1,n) * p1*(1 - p1) + np.arange(1,n)[::-1] * p2 * (1 - p2)
    return res / n
```</p>

<h3 id="hyper-parameters">1.4. Hyper Parameters</h3>

<p>For a tree, the important hyperparameters control its complexity</p>

<ol>
  <li>Max Depth: the lower it is, the less complex. It should not be much higher than \( \log _ 2 n \)</li>
  <li>Min Sample Nodes: each node should contain at least this many samples</li>
  <li>Min Sample split: to consider split a node, the node must have at least this many samples (note that this is different from 2.)</li>
  <li>Max number of nodes: this is straight forward, and is roughly equal to \(2 ^ maxdepth\).</li>
  <li>Criterion: for classification, in addition to Gini score, one can use entropy, which is \(p \log p + (1 - p) \log (1 - p)\). The two functions are in fact very similar</li>
</ol>

<h2 id="ensembles">2. Ensembles</h2>

<h3 id="random-forest">2.1. Random Forest</h3>
<p>Once we understand decision tree, random forest is a piece of cake. It basically do</p>

<ol>
  <li>Sample \(\alpha n, \alpha \in [0,1]\) observations from the original dataset, and \(\beta p, \beta \in [0,1]\) features, and build a tree here</li>
  <li>Repeat the process for k times, and classify observations according to majority, or regression observation according to the mean.</li>
</ol>

<p>Three more hyper parameters that we introduced:</p>

<ol>
  <li>Proportion of observation to sample: the smaller it is, the more independent the different trees are</li>
  <li>Proportion of feature to sample: similar to 1. For classification, it is recommended to be around \(\sqrt{d}\), for regression, it is recommended to be \(d / 3\)</li>
  <li>Number of trees: basically the bigger the better, but just more computation.</li>
</ol>

<h3 id="gradient-boosting-machine">2.2. Gradient Boosting Machine</h3>

<p>The algorithm is as followed (credit ESL)</p>

<ol>
  <li>Initialize \(f _ 0 (x) = \arg \min _ {\gamma} \sum _ {i = 1} L(y _ i, \gamma) \)</li>
  <li>For m = 1 to M:
⋅⋅a. For i = 1, 2, …, N compute 
⋅⋅b. asdsa</li>
</ol>

  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  
  <!-- disqus comments -->
 
 <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'HoangDT Blog'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  
  <div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'andersonvom'; // required: replace example with your forum shortname
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">Hoang Duong blog</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>Hoang Duong blog</li>
        <!-- <li><a href="mailto:hduong@berkeley.edu">hduong@berkeley.edu</a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/hduongtrong">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">hduongtrong</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/hduongtrong">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">hduongtrong</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">Learning Machine Learning</p>
    </div>

  </div>

</footer>


    </body>
</html>